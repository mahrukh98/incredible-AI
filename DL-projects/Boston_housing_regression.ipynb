{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Boston-housing_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hspTROQutedG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regression Housing Pricing Project 3 for Fathers who want to buy a House:  Predicting Housing Prices\n"
      ]
    },
    {
      "metadata": {
        "id": "5KL1TrLzsdGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9537130-9193-46b8-ade8-8f184869ce29"
      },
      "cell_type": "code",
      "source": [
        "# All necessary imports here\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "i_5jFsugvMp9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1. Project Description\n",
        "Now besides the previously attempted classification which included prediction for discrete labels to input data point, in regression problems we're going to predict a continuous value for the median price (in thousands of dollars) of homes in a given Boston suburb  in the mid 1970s. The dataset has  a total of 506 data points out of which 404 are training samples and 102 test samples each with 13 numerical properties/features."
      ]
    },
    {
      "metadata": {
        "id": "ONhjK2mk_nnN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "path_to_data = \"https://raw.githubusercontent.com/mahrukh98/incredible-AI/master/datasets/housing.csv\";\n",
        "# The dataset is in fact not in CSV format, the attributes are instead separated by whitespace. \n",
        "dataframe = pd.read_csv(path_to_data, delim_whitespace=True, header=None)\n",
        "dataset = dataframe.values\n",
        "\n",
        "# Split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HlYXGzBOVg6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2. Develop a Baseline Neural Network Model\n",
        "Creating the baseline model with single, fully connected hidden layer of 13 hidden units (same number as input features) and randomly initialized weights. The hidden layer is passed through 'relu' activation function that zeroes-out the negative values and only keeps the positive values of computation. Output layer with single unit with no activation function, as we're dealing with scalar regression problem- to predict single continuous value Finally, 'mean_squared_error' loss function and 'Adam' optimizer along with recording mean absolute accuracy (mae) metrics are reserved for compilation. "
      ]
    },
    {
      "metadata": {
        "id": "0OXJw3Kouj6Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define base model\n",
        "def baseline_model():\n",
        "  # create model\n",
        "  model = Sequential([\n",
        "      Dense(13, input_shape=(13,)),\n",
        "      Activation('relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "  \n",
        "  #compile model\n",
        "  model.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vh6xycwjbmmN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4MKbox6BadtZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the Keras wrapper object for use in scikit-learn as a regression estimator called KerasRegressor with the model creation function, number of epochs and batch_size as argument."
      ]
    },
    {
      "metadata": {
        "id": "1jwZ6YaoTEIk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "#estimator object instantiation of KerasRegressor wrapper class\n",
        "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okgfJjXccAet",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluating the model using K-fold cross validation, where K = 10"
      ]
    },
    {
      "metadata": {
        "id": "v2sv96EOb_vu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "4f7e165a-26e2-4acd-c26f-2b794c923014"
      },
      "cell_type": "code",
      "source": [
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
        "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Results: -42.49 (23.26) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WbElTKQFdoQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 3: Modeling The Standardized Dataset\n",
        "This dataset consists of 13 input features and all of them have widely different range of values, so data preparation is crucial here as it's always necessary before modelling any neural network.\n",
        "\n",
        "Now, for data preparation we're using the standardization technique from StandardScaler class. We're asked to train the standardization procedure on the training data within the pass of a cross-validation run and to use the trained standardization to prepare the “unseen” test fold using pipeline wrapper!\n"
      ]
    },
    {
      "metadata": {
        "id": "hj1hEtKKxGTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "87e095c4-5a12-447b-b869-fb6997d0ff47"
      },
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: -26.94 (32.53) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4v8gbcH4-_Hm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 4. Tune The Neural Network Topology\n",
        "We'll be tuning the network on the basis of its structure, that can be deeper or wider.\n",
        "\n",
        "\n",
        "\n",
        "> ### Step 4.1. Evaluate a Deeper Network Topology\n",
        "\n",
        "\n",
        "> Deeper network means more layers in the network so as to extract higher order and prominent features from the input data. Here, we're adding 1 more hidden layer with half number of hidden units as in the first layer!  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "85FYxPgc_Fy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# define the deeper model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential([\n",
        "      Dense(13, input_shape=(13,)),\n",
        "      Activation('relu'),\n",
        "      Dense(6),\n",
        "      Activation('relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\treturn model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ojHm2YoBFVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "882ec636-796e-4c5c-fba9-b592cb16cff9"
      },
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=50, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Larger model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger model: -22.51 (28.44) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Axvcd0sCT8d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "> ### Step 4.2. Evaluate a Wider Network Topology\n",
        "\n",
        "\n",
        "> Wider network means more hidden units which might result in increased representational capability. So, here we're increasing the hidden units to be 20 for 1 hidden layer!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "wxq0iyoFCS9l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define wider model\n",
        "def wider_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential([\n",
        "      Dense(20, input_shape=(13,)),\n",
        "      Activation('relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ocpAv9NFk4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ef8a3927-04db-4062-9c6f-875dbd3cdd09"
      },
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Wider model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wider model: -21.26 (21.83) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nflwfslii3xL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This looks pretty good as according to the manual: ***Reasonable performance for models evaluated using Mean Squared Error (MSE) are around 20 in squared thousands of dollars (or $4,500 if you take the square root). ***"
      ]
    },
    {
      "metadata": {
        "id": "bqdhlISIGJww",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 5. Really Scaling up: developing a model that overfits\n",
        "Now, to figure out the strength of our model that it lies exactly right at the border between the underfitting and overfitting, we'll have to cross that border i.e. overfit the model. This can be achieved by:\n",
        "\n",
        "\n",
        "1.   Adding layers ----- 3 layers + 1 output layer\n",
        "2.   Increasing hidden units --- 20----->13------>6------>1\n",
        "3.   Training for more epochs ---100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VAI-Gxs7GMPN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define overfitting model\n",
        "def overfitted_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential([\n",
        "      Dense(20, input_shape=(13,)),\n",
        "      Activation('relu'),\n",
        "      Dense(13),\n",
        "      Activation('relu'),\n",
        "      Dense(6),\n",
        "      Activation('relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hAcq9uvUKXoK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "93f7cfc5-6df0-4436-9a5b-07e0178a17d7"
      },
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=overfitted_model, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Overfitted model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overfitted model: -22.34 (24.57) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9p2QZdzAK3eE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 6. Tuning the Model\n",
        "Here. we're decreasing the number of epochs to be 75. Let's see, whether we we would require any further improvements/tuning after this or not."
      ]
    },
    {
      "metadata": {
        "id": "QAMj_0i_K5aw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define improved model\n",
        "def improved_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential([\n",
        "      Dense(20, input_shape=(13,)),\n",
        "      Activation('relu'),\n",
        "      Dense(13),\n",
        "      Activation('relu'),\n",
        "      Dense(6),\n",
        "      Activation('relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='Adam')\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_PGj0tMbkiJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fe69af7c-ec85-4125-d76f-c40d067718f1"
      },
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=improved_model, epochs=75, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Improved model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved model: -21.14 (21.92) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tFeaMouXa6Y9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 7. Rewriting the code using the Keras Functional API\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9gzmuypwbej0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rnxbjRy9cLi_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "22e388b8-d676-4a73-81a9-6459ee20294e"
      },
      "cell_type": "code",
      "source": [
        "# creating functional API for tuned/improved model \n",
        "def create_improved_fn():\n",
        "  inputs = keras.Input(shape = (13,))\n",
        "  x = layers.Dense(20, activation='relu')(inputs)\n",
        "  x = layers.Dense(13, activation='relu')(x)\n",
        "  x = layers.Dense(6, activation='relu')(x)\n",
        "  output = layers.Dense(1)(x)\n",
        "\n",
        "  model = keras.Model(inputs, output)\n",
        "  \n",
        "  model.compile(loss='mean_squared_error',\n",
        "              optimizer='Adam')\n",
        "  \n",
        "  return model\n",
        "\n",
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=create_improved_fn, epochs=75, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Improved model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved model: -21.14 (21.92) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UFsQ2SskCQA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 8. Rewriting the code by doing Model Subclassing"
      ]
    },
    {
      "metadata": {
        "id": "g7sbpDgtkErO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4L7xjHHkNc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3f71ef5c-9477-439b-9654-5bcf043f8069"
      },
      "cell_type": "code",
      "source": [
        "# Creating subclass for improved/tuned model\n",
        "class Improved(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Improved, self).__init__()\n",
        "    self.dense1 = layers.Dense(20, activation='relu')\n",
        "    self.dense2 = layers.Dense(13, activation='relu')\n",
        "    self.dense3 = layers.Dense(6, activation='relu')\n",
        "    self.dense4 = layers.Dense(1)\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dense3(x)\n",
        "    return self.dense4(x)\n",
        "  \n",
        "# DISCLAIMER!!!\n",
        "# This part is inspired from the functional API style :D \n",
        "# As, build_fn needs callable function or class instance, we're generating another method which will also accompany the input shape not specified in the class and \n",
        "# compilation step\n",
        "\n",
        "def create_Improved_subclass():\n",
        "  inputs = keras.Input(shape = (13,))\n",
        "  model = Improved()\n",
        "  output = model.call(inputs)\n",
        "  \n",
        "  model = keras.Model(inputs, output)\n",
        "  model.compile(loss='mean_squared_error',\n",
        "              optimizer='Adam')\n",
        "  return model\n",
        "\n",
        "# evaluate model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasRegressor(build_fn=create_Improved_subclass, epochs=75, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=10, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
        "print(\"Improved model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved model: -21.14 (21.92) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jGTIID1NmNTi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 9. Rewriting the code without using scikit-learn"
      ]
    },
    {
      "metadata": {
        "id": "eUw_z8GJmNqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating model\n",
        "def create_final_model():\n",
        "  final_model = Sequential([\n",
        "    Dense(20, input_shape=(13,)),\n",
        "    Activation('relu'),\n",
        "    Dense(13),\n",
        "    Activation('relu'),\n",
        "    Dense(6),\n",
        "    Activation('relu'),\n",
        "    Dense(1)\n",
        "   ])\n",
        "  \n",
        "\t# Compiling model\n",
        "  final_model.compile(loss='mean_squared_error',\n",
        "              optimizer='Adam',metrics=['mae'])\n",
        "  return final_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ISUceF0Jnche",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "76a404a2-8741-4791-8917-5d018859f77c"
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "k = 10\n",
        "num_val_samples = len(dataset) // k\n",
        "np.random.shuffle(dataset)\n",
        "all_scores = []\n",
        "# all_trial_s = []\n",
        "num_epochs = 100\n",
        "\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "\n",
        "  # Preparing the validation data and properly partitioning training data\n",
        "  val_X = X[num_val_samples * i:num_val_samples * (i + 1)]\n",
        "  train_X = np.append(X[:num_val_samples * i], X[num_val_samples * (i + 1):], axis=0) \n",
        "    \n",
        "  val_Y = Y[num_val_samples * i:num_val_samples * (i + 1)]\n",
        "  train_Y = np.append(Y[:num_val_samples * i] , Y[num_val_samples * (i + 1):], axis=0)\n",
        "  # Building the Keras model (already compiled)\n",
        "  model = create_final_model() \n",
        "  model.fit(train_X,train_Y,epochs=num_epochs,batch_size=5,verbose=0,validation_data = (val_X,val_Y))\n",
        "  # Evaluate the model on the validation data\n",
        "  val_mse, val_mae = model.evaluate(val_X, val_Y, verbose=0)\n",
        "  all_scores.append(val_mse)\n",
        "  # all_trial_s.append(val_mae)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n",
            "processing fold # 4\n",
            "processing fold # 5\n",
            "processing fold # 6\n",
            "processing fold # 7\n",
            "processing fold # 8\n",
            "processing fold # 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VDGXAqwVVBdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "9ffbf06b-3384-4b17-fe72-529ba3223c15"
      },
      "cell_type": "code",
      "source": [
        "all_scores"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[11.177670936584473,\n",
              " 41.12146514892578,\n",
              " 45.45825225830078,\n",
              " 12.997956085205079,\n",
              " 37.85966926574707,\n",
              " 247.81135620117186,\n",
              " 17.173582458496092,\n",
              " 14.066553840637207,\n",
              " 27.465848541259767,\n",
              " 30.840325775146486]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "metadata": {
        "id": "oQuvYh79hUOA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5aad4dc8-5db8-47ae-e8bf-c94aed74e3e1"
      },
      "cell_type": "code",
      "source": [
        "print(\"Final improved model: %.2f (%.2f) MSE\" % (np.mean(all_scores), np.std(all_scores)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final improved model: 48.60 (67.44) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0TpXiRf3jbA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "237aee8f-6b8e-4790-a1a6-c3ff54da9519"
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "k = 10\n",
        "num_val_samples = len(dataset) // k\n",
        "np.random.shuffle(dataset)\n",
        "all_scores = []\n",
        "# all_trial_s = []\n",
        "num_epochs = 50\n",
        "\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "\n",
        "  # Preparing the validation data and properly partitioning training data\n",
        "  val_X = X[num_val_samples * i:num_val_samples * (i + 1)]\n",
        "  train_X = np.append(X[:num_val_samples * i], X[num_val_samples * (i + 1):], axis=0) \n",
        "    \n",
        "  val_Y = Y[num_val_samples * i:num_val_samples * (i + 1)]\n",
        "  train_Y = np.append(Y[:num_val_samples * i] , Y[num_val_samples * (i + 1):], axis=0)\n",
        "  # Building the Keras model (already compiled)\n",
        "  model = create_final_model() \n",
        "  model.fit(train_X,train_Y,epochs=num_epochs,batch_size=5,verbose=0,validation_data = (val_X,val_Y))\n",
        "  # Evaluate the model on the validation data\n",
        "  val_mse, val_mae = model.evaluate(val_X, val_Y, verbose=0)\n",
        "  all_scores.append(val_mse)\n",
        "  # all_trial_s.append(val_mae)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "processing fold # 1\n",
            "processing fold # 2\n",
            "processing fold # 3\n",
            "processing fold # 4\n",
            "processing fold # 5\n",
            "processing fold # 6\n",
            "processing fold # 7\n",
            "processing fold # 8\n",
            "processing fold # 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PxmkNfHrjkd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "892d4d3d-3fae-4cd3-e0ce-ef7977ee4612"
      },
      "cell_type": "code",
      "source": [
        "print(\"Final improved model: %.2f (%.2f) MSE\" % (np.mean(all_scores), np.std(all_scores)))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final improved model: 27.94 (10.62) MSE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xkhj3CcCnxBa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final model's results are diverged a little because the K-fold cross-validation can be improved and will be for sure!"
      ]
    }
  ]
}