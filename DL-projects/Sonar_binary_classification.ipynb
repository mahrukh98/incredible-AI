{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sonar-binary-classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "z0e_zWOLAVCb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Binary Classification Sonar Project 1 for the Navy:  Mines vs. Rocks\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xML9jGVpRCoI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "96f0d495-ac45-476f-bade-d4d501ce588c"
      },
      "cell_type": "code",
      "source": [
        "# All necessary imports here\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "JCFPQSNMRoHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "np.random.seed(seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RtL4ZivYB2Wf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 1. Description of the Dataset\n",
        "As briefly described in the project manual Sonar dataset consists of mainly 208 observations with 60 input variables and 1 output variable. This project revolves around a binary classification problem as our model has to predict whether or not the detected object is a rock 'R' or a mine 'M'."
      ]
    },
    {
      "metadata": {
        "id": "6a-L_5NWaALi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "path_to_dataset = \"https://raw.githubusercontent.com/mahrukh98/incredible-AI/master/datasets/sonar.csv\"\n",
        "dataframe = pd.read_csv(path_to_dataset, header=None)\n",
        "dataset = dataframe.values\n",
        "# splitting into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-5q0W_Na6wm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "c54af46a-a8da-4938-8be4-43d77df487d9"
      },
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
              "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
              "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
              "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
              "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
              "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
              "       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n",
              "       'R', 'R', 'R', 'R', 'R', 'R', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n",
              "       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "WoOjZVCZE35E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As the output variables need to be numerical so as to be predicted by the model, we're using the Label Encoder class from scikit-learn, then fit_transform method to learn the labels' mean and standard deviation afterwards applying those transformations on the training dataset for encoded labels."
      ]
    },
    {
      "metadata": {
        "id": "fmAy1twuqGs3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#encoder object instantiation of LabelEncoder class\n",
        "encoder = LabelEncoder()\n",
        "#encoded_Y consists of the transformed(scaled) labels\n",
        "encoded_Y = encoder.fit_transform(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ZgsCCbTIyWT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we've received the encoded labels where 1 represents rock and 0 represents mine."
      ]
    },
    {
      "metadata": {
        "id": "l8d4Stuis0PV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "3c05c2f4-9c03-4c88-d6c7-974b2b326347"
      },
      "cell_type": "code",
      "source": [
        "encoded_Y"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "OFQPowvFJ7GD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2. Baseline Neural Network Model Performance\n",
        "Creating the baseline model with single, densely connected hidden layer of 60 hidden units (sames as the input variables or features) and randomly initialized weights. The hidden layer is passed through 'relu' activation function that zeroes-out the negative values and only keeps the positive values of computation. Output layer is passed through 'sigmoid' activation function that squashes the output between 0 and 1 as a probabilistic distribution. Finally, 'binary_crossentropy' loss function which is particular for the binary classification problems , 'Adam' optimizer along with recording accuracy metrics are reserved for compilation."
      ]
    },
    {
      "metadata": {
        "id": "pRvrel01MFR2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# baseline model\n",
        "def create_baseline():\n",
        "  # create model, write code below\n",
        "  model = Sequential([\n",
        "    Dense(60, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'),\n",
        "   ])\n",
        "\t\n",
        "\t# Compile model, write code below\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9k2q7AZjPFwA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, evaluating the model using stratified cross-validation in the scikit-learn framework for that purpose we're using KerasClassifier wrapper class with the model creation function, number of epochs and batch_size as argument. "
      ]
    },
    {
      "metadata": {
        "id": "BAFyZ3V0StZ6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "709b1a45-89bd-423d-8459-4f82871e94bf"
      },
      "cell_type": "code",
      "source": [
        "# evaluate model with standardized dataset\n",
        "#estimator object instantiation of KerasClassifier wrapper class\n",
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
        "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Results: 83.71% (6.13%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6-8qVKi_W5Bg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 3. Re-Run The Baseline Model With Data Preparation\n",
        "Now, for data preparation we're using the standardization technique from StandardScaler class.\n",
        "We're asked to train the standardization procedure on the training data within the pass of a cross-validation run and to use the trained standardization to prepare the “unseen” test fold using pipeline wrapper!"
      ]
    },
    {
      "metadata": {
        "id": "vX695HcWg_uf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5d70ddd4-4c55-48b7-8589-4add105b5a45"
      },
      "cell_type": "code",
      "source": [
        "# evaluate baseline model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "#scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: 85.59% (7.46%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gc647rRlaOvJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 4. Tuning Layers and Number of Neurons in The Model\n",
        "\n",
        "\n",
        "> **4.1. Evaluate a Smaller Network** Let's see, what reducing the hidden units to 30 adds to our model's capability!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "F6oNiIlTZ_NU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7be69e64-f501-4ca8-cbc1-33c9afe4b5a5"
      },
      "cell_type": "code",
      "source": [
        "# smaller model\n",
        "def create_smaller():\n",
        "\t# Create model\n",
        "\tmodel = Sequential([\n",
        "    Dense(30, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'),\n",
        "   ])\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "# standardizing the data with data preparation during the k-fold cross-validation run\n",
        "estimators = []\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Smaller: 83.09% (6.41%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Flpv3lide4iD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "> ### 4.2. Evaluate a Larger Network \n",
        "Let's try another hidden layer of 30 hidden units after the first one with 60 hidden units!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "4ScoaimyMIkf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "82be8e6d-f1d4-4869-b36b-9560db35d7f6"
      },
      "cell_type": "code",
      "source": [
        "# larger model\n",
        "def create_larger():\n",
        "\t# create model\n",
        "\tmodel = Sequential([\n",
        "    Dense(60, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(30, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'),\n",
        "   ])\n",
        "  \n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "estimators = []\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger: 85.09% (7.25%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZxA5td5Ie4lv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 5: Really Scaling up: developing a model that overfits \n",
        "Now, to figure out the strength of our model that it lies exactly right at the border between the underfitting and overfitting, we'll have to cross that border i.e. overfit the model. This can be achieved by:\n",
        "\n",
        "\n",
        "1.   Adding layers ----- 4 layers + 1 output layer\n",
        "2.   Increasing hidden units --- 120----->60------>30------>15------>1\n",
        "3.   Training for more epochs ---150\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RYhn9QDzdVKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b55ebee-d263-485f-d2f2-1bf7dd8f7af9"
      },
      "cell_type": "code",
      "source": [
        "# overfitted model\n",
        "def create_overfitted_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential([\n",
        "    Dense(120, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(60, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(30, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(15, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'),\n",
        "   ])\n",
        "  \n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "estimators = []\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_overfitted_model, epochs=150, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"New overfitted model: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New overfitted model: 87.97% (7.54%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qWlGv8sgsNZo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 6: Tuning the Model \n",
        "Changing the optimizer from 'Adam' to 'rmsprop' here, though we can tune many other hyper-parameters as well."
      ]
    },
    {
      "metadata": {
        "id": "wPRzwFdu4WVP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "89ec93c7-de19-4981-8e01-793966460d7d"
      },
      "cell_type": "code",
      "source": [
        "# improved model\n",
        "def create_model():\n",
        "\t# create model\n",
        "\tmodel = Sequential([\n",
        "    Dense(120, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(60, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(30, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(15, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'),\n",
        "   ])\n",
        "  \n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "estimators = []\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=120, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Improved model: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved model: 85.97% (8.04%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LisDuByD3KDL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 7: Rewriting the code using the Keras Functional API"
      ]
    },
    {
      "metadata": {
        "id": "HcbrJRY4_hV6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nOepBaBHB8mJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "578b66e9-4442-4041-a6f2-84397092b9c9"
      },
      "cell_type": "code",
      "source": [
        "# creating functional API for baseline model \n",
        "def create_baseline_fn():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  x = layers.Dense(60, activation='relu')(inputs)\n",
        "  output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = keras.Model(inputs, output)\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# evaluate baseline model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline_fn, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Baseline Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Standardized model accuracy: 85.59% (7.46%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "94mBaUaTDoLr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "046cc774-5e2f-4294-ad98-dc53c6541f3d"
      },
      "cell_type": "code",
      "source": [
        "# creating functional API for smaller model \n",
        "def create_smaller_fn():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  x = layers.Dense(30, activation='relu')(inputs)\n",
        "  output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = keras.Model(inputs, output)\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# evaluate smaller model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller_fn, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Smaller Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Smaller Standardized model accuracy: 85.06% (7.61%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P6wTL7flFBBg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8c1352b6-bcc2-421d-d178-aeee55c5322d"
      },
      "cell_type": "code",
      "source": [
        "# creating functional API for larger model \n",
        "def create_larger_fn():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  x = layers.Dense(60, activation='relu')(inputs)\n",
        "  x = layers.Dense(30, activation='relu')(x)\n",
        "  output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = keras.Model(inputs, output)\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# evaluate larger model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_larger_fn, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Larger Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger Standardized model accuracy: 84.61% (6.36%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AMMi2W4y3PgX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a14d845b-f34b-416f-962d-cf6dc6924452"
      },
      "cell_type": "code",
      "source": [
        "# creating functional API for overfitted model \n",
        "def create_overfitted_fn():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  x = layers.Dense(120, activation='relu')(inputs)\n",
        "  x = layers.Dense(60, activation='relu')(x)\n",
        "  x = layers.Dense(30, activation='relu')(x)\n",
        "  x = layers.Dense(15, activation='relu')(x)\n",
        "  output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = keras.Model(inputs, output)\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# evaluate overfitted model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn= create_overfitted_fn, epochs=150, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Overfitted Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overfitted Standardized model accuracy: 87.45% (6.66%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q0PAdvuaH6Bt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ec82b77-de21-40ad-cd86-ac1f5373574d"
      },
      "cell_type": "code",
      "source": [
        "# creating functional API for tuned/improved model \n",
        "def create_improved_fn():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  x = layers.Dense(120, activation='relu')(inputs)\n",
        "  x = layers.Dense(60, activation='relu')(x)\n",
        "  x = layers.Dense(30, activation='relu')(x)\n",
        "  x = layers.Dense(15, activation='relu')(x)\n",
        "  output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "  model = keras.Model(inputs, output)\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "# evaluate improved model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn= create_improved_fn, epochs=120, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Improved Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved Standardized model accuracy: 87.02% (7.70%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nBAPN2OnLWm7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 8: Rewriting the code by doing Model Subclassing"
      ]
    },
    {
      "metadata": {
        "id": "eJ_NlcdbLR5q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lqvr1zZSMJSh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7945db2d-061b-40cc-c6e1-a32dcefd8bb3"
      },
      "cell_type": "code",
      "source": [
        "# Creating subclass for baseline model\n",
        "class Baseline(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Baseline, self).__init__()\n",
        "    self.dense1 = layers.Dense(60, activation='relu')\n",
        "    self.dense2 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    return self.dense2(x)\n",
        "  \n",
        "# DISCLAIMER!!!\n",
        "# This part is inspired from the functional API style :D \n",
        "# As, build_fn needs callable function or class instance, we're generating another method which will also accompany the input shape not specified in the class and \n",
        "# compilation step\n",
        "\n",
        "def create_Baseline_subclass():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  model = Baseline()\n",
        "  output = model.call(inputs)\n",
        "  \n",
        "  model = keras.Model(inputs, output)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# evaluate baseline model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn= create_Baseline_subclass, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Baseline Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline Standardized model accuracy: 85.59% (7.46%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g_HrxQ_cgJlx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4f7ef092-a45c-4377-d2da-8e5e8459caed"
      },
      "cell_type": "code",
      "source": [
        "# Creating subclass for smaller model\n",
        "class Smaller(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Smaller, self).__init__()\n",
        "    self.dense1 = layers.Dense(30, activation='relu')\n",
        "    self.dense2 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    return self.dense2(x)\n",
        "  \n",
        "# DISCLAIMER!!!\n",
        "# This part is inspired from the functional API style :D \n",
        "# As, build_fn needs callable function or class instance, we're generating another method which will also accompany the input shape not specified in the class and \n",
        "# compilation step\n",
        "\n",
        "def create_Smaller_subclass():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  model = Smaller()\n",
        "  output = model.call(inputs)\n",
        "  \n",
        "  model = keras.Model(inputs, output)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# evaluate smaller model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn= create_Smaller_subclass, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Smaller Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Smaller Standardized model accuracy: 85.06% (7.61%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XlBDzhxlhRr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "55974e9a-f338-4c57-a5f5-c4c59697e873"
      },
      "cell_type": "code",
      "source": [
        "# Creating subclass for larger model\n",
        "class Larger(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Larger, self).__init__()\n",
        "    self.dense1 = layers.Dense(60, activation='relu')\n",
        "    self.dense2 = layers.Dense(30, activation='relu')\n",
        "    self.dense3 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    return self.dense3(x)\n",
        "  \n",
        "# DISCLAIMER!!!\n",
        "# This part is inspired from the functional API style :D \n",
        "# As, build_fn needs callable function or class instance, we're generating another method which will also accompany the input shape not specified in the class and \n",
        "# compilation step\n",
        "\n",
        "def create_Larger_subclass():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  model = Larger()\n",
        "  output = model.call(inputs)\n",
        "  \n",
        "  model = keras.Model(inputs, output)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# evaluate larger model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn= create_Larger_subclass, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Larger Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger Standardized model accuracy: 84.61% (6.36%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VyvIN0_yjC2w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c2405731-6d76-4984-a198-4a8b46a98651"
      },
      "cell_type": "code",
      "source": [
        "# Creating subclass for overfitting model\n",
        "class Overfitted(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Overfitted, self).__init__()\n",
        "    self.dense1 = layers.Dense(120, activation='relu')\n",
        "    self.dense2 = layers.Dense(60, activation='relu')\n",
        "    self.dense3 = layers.Dense(30, activation='relu')\n",
        "    self.dense4 = layers.Dense(15, activation='relu')\n",
        "    self.dense5 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dense3(x)\n",
        "    x = self.dense4(x)\n",
        "    return self.dense5(x)\n",
        "  \n",
        "# DISCLAIMER!!!\n",
        "# This part is inspired from the functional API style :D \n",
        "# As, build_fn needs callable function or class instance, we're generating another method which will also accompany the input shape not specified in the class and \n",
        "# compilation step\n",
        "\n",
        "def create_Overfitted_subclass():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  model = Overfitted()\n",
        "  output = model.call(inputs)\n",
        "  \n",
        "  model = keras.Model(inputs, output)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# evaluate overfitted model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn= create_Overfitted_subclass, epochs=150, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Overfitting Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overfitting Standardized model accuracy: 87.45% (6.66%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qt0mG7K-kzhJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56967e12-15ac-4bc4-96e3-aae0bf3e2a81"
      },
      "cell_type": "code",
      "source": [
        "# Creating subclass for improved/tuned model\n",
        "class Improved(keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Improved, self).__init__()\n",
        "    self.dense1 = layers.Dense(120, activation='relu')\n",
        "    self.dense2 = layers.Dense(60, activation='relu')\n",
        "    self.dense3 = layers.Dense(30, activation='relu')\n",
        "    self.dense4 = layers.Dense(15, activation='relu')\n",
        "    self.dense5 = layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense1(inputs)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dense3(x)\n",
        "    x = self.dense4(x)\n",
        "    return self.dense5(x)\n",
        "  \n",
        "# DISCLAIMER!!!\n",
        "# This part is inspired from the functional API style :D \n",
        "# As, build_fn needs callable function or class instance, we're generating another method which will also accompany the input shape not specified in the class and \n",
        "# compilation step\n",
        "\n",
        "def create_Improved_subclass():\n",
        "  inputs = keras.Input(shape = (60,))\n",
        "  model = Improved()\n",
        "  output = model.call(inputs)\n",
        "  \n",
        "  model = keras.Model(inputs, output)\n",
        "  model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "# evaluate improved model with standardized dataset\n",
        "np.random.seed(seed)\n",
        "estimators = []\n",
        "# scaler object instantiation of StandardScaler class\n",
        "scaler = StandardScaler()\n",
        "estimators.append(('standardize', scaler))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn= create_Improved_subclass, epochs=120, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Improved Standardized model accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Improved Standardized model accuracy: 86.99% (6.80%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r5gnczOym4Yc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 9: Rewriting the code without using scikit-learn\n",
        "Here, we're asked for selecting the best model and implementing its code + the k-fold cross validation without scikit-learn !"
      ]
    },
    {
      "metadata": {
        "id": "_L2nHRTGwZNp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Converting string labels into one-hot encoded labels either 0 or 1\n",
        "encoded_labels = []\n",
        "for i in range(len(Y)):\n",
        "  if Y[i] == 'R':\n",
        "    encoded_labels.append(1)\n",
        "  else:\n",
        "    encoded_labels.append(0)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zrPEYEFA3AVp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "cd046d80-c5a4-4abb-dfaa-3a4f63eb06d0"
      },
      "cell_type": "code",
      "source": [
        "encoded_labels_arr = np.array(encoded_labels)\n",
        "encoded_labels_arr"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "PgGtg5aql7Fs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating model\n",
        "def create_final_model():\n",
        "  final_model = Sequential([\n",
        "    Dense(120, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(60, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(30, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(15, input_shape=(60,)),\n",
        "    Activation('relu'),\n",
        "    Dense(1),\n",
        "    Activation('sigmoid'),\n",
        "   ])\n",
        "  \n",
        "\t# Compiling model\n",
        "  final_model.compile(loss='binary_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "  return final_model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8W6AwoZ-325P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Applying K-fold cross validation here"
      ]
    },
    {
      "metadata": {
        "id": "C_EEqc4v1MRe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "e461219d-8160-4691-bfad-2fc6e636dc55"
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(seed)\n",
        "k = 10\n",
        "num_val_samples = len(dataset) // k\n",
        "np.random.shuffle(dataset)\n",
        "all_scores = []\n",
        "num_epochs = 100\n",
        "\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "\n",
        "  # Preparing the validation data and properly partitioning training data\n",
        "  val_X = X[num_val_samples * i:num_val_samples * (i + 1)]\n",
        "  train_X = np.append(X[:num_val_samples * i], X[num_val_samples * (i + 1):], axis=0) \n",
        "    \n",
        "  val_Y = encoded_labels_arr[num_val_samples * i:num_val_samples * (i + 1)]\n",
        "  train_Y = np.append(encoded_labels_arr[:num_val_samples * i] , encoded_labels_arr[num_val_samples * (i + 1):], axis=0)\n",
        "  # Building the Keras model (already compiled)\n",
        "  model = create_final_model() \n",
        "  all_scores = model.fit(train_X,train_Y,epochs=num_epochs,batch_size=5,verbose=0,validation_data = (val_X,val_Y))\n",
        "  \n",
        "  # Saving state dictionary of model    \n",
        "  history_dict = all_scores.history\n",
        "  val_score = np.average(history_dict['val_acc'])\n",
        "\n",
        "  print(\"Final improved model's accuracy: %.2f%% \" % (val_score*100))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Final improved model's accuracy: 35.15% \n",
            "processing fold # 1\n",
            "Final improved model's accuracy: 60.15% \n",
            "processing fold # 2\n",
            "Final improved model's accuracy: 55.05% \n",
            "processing fold # 3\n",
            "Final improved model's accuracy: 76.80% \n",
            "processing fold # 4\n",
            "Final improved model's accuracy: 53.15% \n",
            "processing fold # 5\n",
            "Final improved model's accuracy: 25.75% \n",
            "processing fold # 6\n",
            "Final improved model's accuracy: 49.70% \n",
            "processing fold # 7\n",
            "Final improved model's accuracy: 64.45% \n",
            "processing fold # 8\n",
            "Final improved model's accuracy: 47.25% \n",
            "processing fold # 9\n",
            "Final improved model's accuracy: 98.60% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UKbm00WcsWJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39f93f1b-b630-4506-a2f9-0f07e5569cf1"
      },
      "cell_type": "code",
      "source": [
        "val_score.mean()*100"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.60000003129244"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}